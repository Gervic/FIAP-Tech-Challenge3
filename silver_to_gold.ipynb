{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import *\nfrom functools import reduce",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "def read_s3_file(bucket_name:str, file_path:str)-> DataFrame:\n  \"\"\"\n  Function used to read files from a s3 bucket\n  args:\n    bucket_name: Name of the s3 bucket\n    file_path: Path of the file to be read\n  returns:\n    DataFrame with the file content\n  \"\"\"\n  return spark.read.parquet(f\"s3a://{bucket_name}/{file_path}/\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "silver_bucket_name = 'project-pnad-covid-ibge-silver-layer'\ngold_bucket_name = 'project-pnad-covid-ibge-gold-layer'\npath_silver_df = 'pnad_covid_datasets_cleaned_transformed_unified'\npath_gold_df = 'pnad_covid_datasets_aggregated'\ncatalogDatabase = 'big_data_project'\ncatalogTableName = 'curated_pnad_covid_datasets'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "silver_df = spark.read.parquet(f\"s3a://{silver_bucket_name}/{path_silver_df}/\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "silver_agregated = (silver_df\n                     .groupBy('month', \n                              'unidade_federacao', \n                              'capital_uf', \n                             )\n                    .pivot(\"question_description\")\n                    .agg(collect_list('question_answer').alias('question_answer'))\n                )",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cols_to_considered = [\n                        'teve_febre',\n                        'teve_tosse',\n                        'teve_dor_no_peito',\n                        'teve_dor_de_garganta',\n                        'teve_dor_de_cabeca',\n                        'teve_nausea',\n                        'teve_dificuldade_para_respirar',\n                        'teve_nariz_entupido_ou_escorrendo',\n                        'teve_fadiga',\n                        'teve_dor_nos_olhos',\n                        'teve_perda_de_cheiro_ou_sabor',\n                        'teve_dor_muscular',\n                        'teve_diarreia',\n                        'recuperacao_sintomas_em_casa',\n                        'providencia_recuperacao_sintomas_procurar_profissional_saude',\n                        'recuperacao_sintomas_por_remedios_auto_medicado',\n                        'recuperacao_sintomas_por_remedios_medicado',\n                        'recuperacao_sintomas_por_sus_a_domicilio',\n                        'recuperacao_sintomas_por_medico_particular_a_domicilio',\n                        'providencia_recuperacao_sintomas_foi_outra',\n                        'buscou_atendimento_ubs',\n                        'buscou_atendimento_ps_sus_upa',\n                        'buscou_atendimento_hospital_sus',\n                        'buscou_atendimento_ambulantorio_forcas_armadas',\n                        'buscou_atendimento_ps_privado_forcas_armadas',\n                        'ficou_internado_por_1dia_ou_mais',\n                        'foi_sedado_entubado_com_respiracao_artificial',\n                        'tem_plano_de_saude',\n                        'fez_algum_test_covid',\n                        'fez_exame_cotonete_swab',\n                        'fez_exame_sangue_do_furo_no_dedo',\n                        'fez_exame_sangue_veia_braco',\n                        'ja_teve_diagnostico_diabetes',\n                        'ja_teve_diagnostico_hipertensao',\n                        'ja_teve_doenca_respiratoria',\n                        'ja_teve_diagnostico_depressao',\n                        'ja_teve_diagnostico_cancer',\n                        'ja_teve_diagnostico_doencas_coracao',\n                        'trabalhou_fez_algum_bico_na_semana_anterior',\n                        'ficou_afastado_temporiaramente_do_trabalho_na_semana_anterior',\n                        'foi_remunerado_nesse_periodo',\n                        'tem_mais_de_um_trabalho',\n                        'carteira_assinada_ou_funcionario_estatutario',\n                        'home_office_na_semana_passada',\n                        'contribuidor_inss',\n                        'fez_emprestimo_na_pandemia',\n                        'emprestimo_em_banco_ou_financeira',\n                        'tem_itens_basico_de_limpeza_em_casa', \n                        'resultado_exame_swab', \n                        'resultado_exame_sangue_do_furo_no_dedo', \n                        'resultado_exame_sangue_veia_braco',\n                        'tipo_de_area',\n                         'cor_ou_raca',\n                         'sexo',\n                         'faixa_etaria',\n                         'setor_da_empresa_do_trabalho',\n                         'trabalho_setor_privado_ou_publico',\n                         'tipo_trabalho_ou_cargo',\n                         'escolaridade',\n                         'tipo_escola_faculdade',\n                         'situacao_do_domicilio',\n                         'idade_do_morador'\n                    ]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_list = []\nfor c in cols_to_considered:\n    try:\n        df = (silver_agregated\n              .select('month', \n                      'unidade_federacao', \n                      'capital_uf', \n                      explode(col(c)).alias(c)\n                     )\n            )\n        df_list.append(df)\n    except:\n        continue",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def merge_df(list_df: list):\n    return reduce(lambda df1, df2: df1.join(df2, ['month', 'unidade_federacao', 'capital_uf'], 'full'), list_df)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_gold = merge_df(df_list)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "list_cols_to_aggregated2 = ['resultado_exame_swab', \n                            'resultado_exame_sangue_do_furo_no_dedo', \n                            'resultado_exame_sangue_veia_braco'\n                            ]\nto_ignore = ['tipo_de_area',\n             'cor_ou_raca',\n             'sexo',\n             'faixa_etaria',\n             'setor_da_empresa_do_trabalho',\n             'trabalho_setor_privado_ou_publico',\n             'tipo_trabalho_ou_cargo',\n             'escolaridade',\n             'tipo_escola_faculdade',\n             'situacao_do_domicilio',\n              'idade_do_morador',\n             'month', \n             'unidade_federacao', \n             'capital_uf'\n        ]\n\nlist_cols_to_aggregated = [c for c in df_gold.columns if c not in list_cols_to_aggregated2 and c not in to_ignore]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "agg_list = []\nfor c in list_cols_to_aggregated:\n    case_sim = when(col(c) == 'sim', 1).otherwise(0)\n    case_nao = when(col(c) == 'nao', 1).otherwise(0)\n    case_nao_sabe = when(col(c) == 'nao_sabe', 1).otherwise(0)\n    case_nao_aplicavel = when(col(c) == 'nao_aplicavel', 1).otherwise(0)\n\n    agg_list.append(sum(case_sim).alias(f'qtd_pessoas_sim_para_{c}'))\n    agg_list.append(sum(case_nao).alias(f'qtd_pessoas_nao_para_{c}'))\n    agg_list.append(sum(case_nao_sabe).alias(f'qtd_pessoas_nao_sabe_para_{c}'))\n    agg_list.append(sum(case_nao_aplicavel).alias(f'qtd_pessoas_nao_aplicavel_para_{c}'))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "for c in list_cols_to_aggregated2:\n    case_positivo = when(col(c) == 'Positivo', 1).otherwise(0)\n    case_negativo = when(col(c).isin('Negativo', 'Inconclusivo'), 1).otherwise(0)\n    agg_list.append(sum(case_positivo).alias(f'qtd_pessoas_positivo_para_{c}'))\n    agg_list.append(sum(case_negativo).alias(f'qtd_pessoas_negativo_para_{c}'))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_gold_aggregated = (df_gold\n                    .withColumn('faixa_etaria', when(col('idade_do_morador').cast('int') <= 25, '0-18')\n                                .when((col('idade_do_morador').cast('int') > 18) & (col('idade_do_morador').cast('int') <= 35), '18-35')\n                                .when((col('idade_do_morador').cast('int') > 35) & (col('idade_do_morador').cast('int') <= 45), '36-45')\n                                .when((col('idade_do_morador').cast('int') > 45) & (col('idade_do_morador').cast('int') <= 55), '46-55')\n                                .when((col('idade_do_morador').cast('int') > 55) & (col('idade_do_morador').cast('int') <= 65), '56-65')\n                                .when(col('idade_do_morador').cast('int') > 65, '65+')\n                                .otherwise('Unknown')\n                                )\n                        .groupBy('month', \n                                 'unidade_federacao', \n                                 'tipo_de_area',\n                                 'cor_ou_raca',\n                                 'sexo',\n                                 'faixa_etaria',\n                                 'setor_da_empresa_do_trabalho',\n                                 'trabalho_setor_privado_ou_publico',\n                                 'tipo_trabalho_ou_cargo',\n                                 'escolaridade',\n                                 'tipo_escola_faculdade',\n                                 'situacao_do_domicilio'\n                                )\n                        .agg(*agg_list)\n                    )                       ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df_gold_aggregated.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JJavaError: An error occurred while calling o6934.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 71 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason:\norg.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 2 partition 7\n\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1995)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:237)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:505)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:467)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\ndynamic_frame = DynamicFrame.fromDF(df_gold_aggregated, glueContext, \"dynamic_frame\")\n\ns3output = glueContext.getSink(\n  path = f\"s3://{gold_bucket_name}/{path_gold_df}\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",           # Important for catalog creation\n  partitionKeys=['month'],                       # Optional: specify if you want partitions\n  compression=\"snappy\",                          # Recommended for Parquet\n  enableUpdateCatalog=True,                      # This triggers Glue Catalog update/create\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=catalogDatabase,               # Glue database (must exist)\n  catalogTableName=catalogTableName              # Table will be created if not existing\n)\n\ns3output.setFormat(\"glueparquet\")                # Recommended format for Glue compatibility\ns3output.writeDynamicFrame(dynamic_frame) ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}