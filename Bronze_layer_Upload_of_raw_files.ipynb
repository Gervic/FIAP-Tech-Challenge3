{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following notebook is part of data pipeline project using the medallion architecture. \n",
        "In this notebook, we have created a s3 bucket and upload the project datasets using the aws boto3 library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vCAxr2Dw5bEe"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import DataFrame\n",
        "from functools import reduce\n",
        "import findspark\n",
        "import boto3\n",
        "import os\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vg4S5zlX4LDp"
      },
      "outputs": [],
      "source": [
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"IUESG4784AFDSGUIGHFSED\"\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"umgD/iCR+JHFAEGVHGFDSBJHrtejusdz3JVYCdxkeYzz\"\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4gSPI_poAijN"
      },
      "outputs": [],
      "source": [
        "aws_access_key_id = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region_name = os.environ.get(\"AWS_DEFAULT_REGION\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LeY-87njAjBI"
      },
      "outputs": [],
      "source": [
        "spark = (SparkSession.builder\n",
        "        .appName(\"ColabS3Upload\")\n",
        "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "        .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key_id)\n",
        "        .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_access_key)\n",
        "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
        "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\")\n",
        "        .getOrCreate()\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cvxyFu7s54ml"
      },
      "outputs": [],
      "source": [
        "def read_dataset(path:str, comma:bool=True)-> DataFrame:\n",
        "  sep = ',' if comma else ';'\n",
        "  df = spark.read.csv(path, sep=sep, header=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rBtb8Ngv6vSM"
      },
      "outputs": [],
      "source": [
        "def create_bucket(bucket_name:str):\n",
        "  s3 = boto3.client(\n",
        "      's3',\n",
        "      aws_access_key_id= aws_access_key_id,\n",
        "      aws_secret_access_key= aws_secret_access_key,\n",
        "      region_name=region_name\n",
        "  )\n",
        "  s3.create_bucket(Bucket=bucket_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S6wdpnCtAu3D"
      },
      "outputs": [],
      "source": [
        "bucket_name = 'project-pnad-covid-bronze-layer'\n",
        "create_bucket(bucket_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3Vr-fxgE9yCt"
      },
      "outputs": [],
      "source": [
        "path_2020_09 = '/content/PNAD_COVID_092020.csv'\n",
        "path_2020_10 = '/content/PNAD_COVID_102020.csv'\n",
        "path_2020_11 = '/content/PNAD_COVID_112020.csv'\n",
        "path_dict = '/content/dicionario_PNAD_COVID_112020_20220621.csv'\n",
        "\n",
        "# Reading project datasets\n",
        "df_2020_09 = read_dataset(path = path_2020_09)\n",
        "df_2020_10 = read_dataset(path = path_2020_10)\n",
        "df_2020_11 = read_dataset(path = path_2020_11)\n",
        "df_dict = read_dataset(path = path_dict, comma = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5QpbDRRc-Fy3"
      },
      "outputs": [],
      "source": [
        "df_2020_09.write.mode(\"overwrite\").parquet(f\"s3a://{bucket_name}/PNAD_COVID_092020_parquet/\")\n",
        "df_2020_10.write.mode(\"overwrite\").parquet(f\"s3a://{bucket_name}/PNAD_COVID_102020_parquet/\")\n",
        "df_2020_11.write.mode(\"overwrite\").parquet(f\"s3a://{bucket_name}/PNAD_COVID_112020_parquet/\")\n",
        "df_dict.write.mode(\"overwrite\").parquet(f\"s3a://{bucket_name}/dicionario_PNAD_COVID_parquet/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6tm14PisClW0"
      },
      "outputs": [],
      "source": [
        "df_dict.write.mode(\"overwrite\").parquet(f\"s3a://{bucket_name}/dicionario_PNAD_COVID_parquet/\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMNoqLWH7qu2ThfonCDBVat",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
